{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9b1b608-14e2-4cb0-97ea-e775c7f4cc7d",
   "metadata": {},
   "source": [
    "# Cleaning Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c7f777-1807-41fe-8d3d-84b194bd1843",
   "metadata": {},
   "source": [
    "Often, data you read in has problems. These problems can include missing data, entry errors, and type errors. Some of these issues are really filtering problems. For instance, a person whose weight is listed as 1555 pounds is probably a 155 lb person but the person entering data hit \"5\" one too many times. This sort of issue is best addressed by filtering out any weight that is too extreme, which will involve our next topic, filtering. (And, potentially, summary statistics to detect outliers.) In this section we will cover more basic data cleaning, such as dropping rows with empty data. In fact, that is exactly where we will start. The code below will create the dataframe from the last notebook but now with some data missing and some other lines duplicated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19cd41f9",
   "metadata": {},
   "source": [
    "### Dropping Blank Cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa7a43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "workout_dict = {'ID': [], 'Measurement Device': [], 'Heart Rate Max': [], 'Heart Rate Min': [], 'Heart Rate Avg': [],\n",
    "              'Duration of exercise (min)': [], 'Exercise Type': []}\n",
    "used_ids = []\n",
    "\n",
    "for x in range(0, 500):\n",
    "    id = random.randint(100000000, 999999999)\n",
    "    while id in used_ids:\n",
    "        id = random.randint(100000000, 999999999)\n",
    "    used_ids.append(id)\n",
    "    device = random.choice(['Skykandal', 'B-Wolf'])\n",
    "    mu = random.randint(65, 85)\n",
    "    min_rate = int(random.gauss(mu, 10))\n",
    "    max_rate = int(random.gauss(mu + 55, 25))\n",
    "    while max_rate <= min_rate:\n",
    "        max_rate = int(random.gauss(mu + 55, 25))\n",
    "    avg = random.gauss((max_rate + min_rate) / 2, (max_rate - min_rate) / 5)\n",
    "    duration = random.randint(10, 90)\n",
    "    exercise = random.choice(['Running', 'Running', 'Running', 'Bicycling', 'Swimming', 'Swimming',\n",
    "                              'Weight training'])\n",
    "    row = [device, min_rate, max_rate, avg, duration, exercise]\n",
    "    # make blank cells, always make a blank on row 3 where we can see it\n",
    "    if x == 2 or random.randint(0, 3) == 0:\n",
    "        i = random.randint(0, 5)\n",
    "        row.pop(i)\n",
    "        row.insert(i, np.nan)\n",
    "        # chance that more cells will be blank\n",
    "        additional_blank = random.randint(0, 3)\n",
    "        while additional_blank == 0:\n",
    "            i = random.randint(0, 5)\n",
    "            row.pop(i)\n",
    "            row.insert(i, np.nan)\n",
    "            additional_blank = random.randint(0, 3)\n",
    "    # very rarely, but always on row 31, the whole row will be missing data\n",
    "    elif random.randint(0, 50) == 0 or x == 30:\n",
    "        id = np.nan\n",
    "        row = [np.nan, np.nan, np.nan, np.nan, np.nan, np.nan]\n",
    "    workout_dict['ID'].append(id)\n",
    "    workout_dict['Measurement Device'].append(row[0])\n",
    "    workout_dict['Heart Rate Min'].append(row[1])\n",
    "    workout_dict['Heart Rate Max'].append(row[2])\n",
    "    workout_dict['Heart Rate Avg'].append(row[3])\n",
    "    if row[4] > 0:\n",
    "        workout_dict['Duration of exercise (min)'].append(str(row[4]))\n",
    "    else:\n",
    "        workout_dict['Duration of exercise (min)'].append(row[4])\n",
    "    workout_dict['Exercise Type'].append(row[5])\n",
    "\n",
    "df = pd.DataFrame(workout_dict)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b70951",
   "metadata": {},
   "source": [
    "In the output above you will see one or more cells that read `NaN` rather than a value. `NaN` is \"Not a Number\" and it is how pandas displays missing values. In this case, your data is generated randomly so I don't know exactly how many values are missing (although there will always be something missing on row three), but given the code that generated it roughly one-quarter of all rows should be missing a value.\n",
    "\n",
    "First, how would we detect missing values? Dataframes have an `isna` method that returns either a `True` (the cell is blank) or a `False` (the cell has a value). Let's run this on row three (index 2, which is coded to always have at least one `NaN` value)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d67537",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[2].isna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447c4487",
   "metadata": {},
   "source": [
    "What you should see is that for any column that is `NaN` you got a `True` and for the other columns you got `False`. We could also run this on the whole dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f50b8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a49d0885",
   "metadata": {},
   "source": [
    "This is less useful, since we now have the entire dataframe turned into `True` and `False`, but using `sum` will help. This will treat every `True` as 1 and every `False` as 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e0f1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6fd7907",
   "metadata": {},
   "source": [
    "This should provide you with column names followed by a number. That number represents the number of `NaN`s in that row. One quick sanity check is that the number of `NaN`s in ID should be much lower than the other rows, since the generator that made this data will never give ID a null value unless the whole row is null.\n",
    "\n",
    "This method has let us grasp the scope of the problem. Let's start getting rid of `NaN`s.\n",
    "\n",
    "The key method here is `dropna`. `dropna` takes a few arguments. We won't cover all of them, since some are for more advanced usage, but the essential ones are `axis`, `how`, and `inplace`. `axis` defaults to 0, which means `dropna` will act row-by-row. 1 would be column-by-column. `how` can take one of two values: `any`, which drops any row or column (based on the value of `axis`) which has any `NaN`s, or `all` which drops a row/column only if it is all `NaN`s. It defaults to `any`. Our old friend `inplace` functions as before: `True` modifies the current dataframe, `False` makes a new, modified, dataframe. It defaults to `False`.\n",
    "\n",
    "So, let's drop any row which is all `NaN`s. That's a garbage row, and I doubt you'll ever see such a thing in the All of Us data. In this case, we'll be using `inplace=False` so we don't alter our dataframe, and can see what different sorts of drops would do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21cb4cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this could also be written:\n",
    "# df.dropna(axis=0, how='all', inplace=False)\n",
    "# however, axis=0 and inplace=False are the defaults, so we can just not write those arguments\n",
    "df_dropped = df.dropna(how='all')\n",
    "print(df_dropped['ID'].isna().sum())\n",
    "print(len(df_dropped['Heart Rate Max']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "333a712c",
   "metadata": {},
   "source": [
    "You should see two numbers printed above. The first should be zero, and the second should be less than 500. In fact, it should be less than 500 by the same amount as the number of `NaN`s in ID, because `NaN` only appears in ID if the whole row is `NaN`. That second number is the size of the data set once we drop the all-`NaN` rows.\n",
    "\n",
    "What would happen if we changed the axis?\n",
    "\n",
    "#### Try this below, using `axis=1` and `how='all'`. Use `head` on your new dataframe to see what it looks like now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4766589",
   "metadata": {},
   "outputs": [],
   "source": [
    "# put your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c9a0e96",
   "metadata": {},
   "source": [
    "This doesn't look any different from the original dataframe. Why? Because there's no column that is entirely `NaN`s. So, let's modify the code to use `how='any'`. Since this is the default option we could just leave the `how` argument off, but I'll specify it here because that's clearer to read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b068552d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2 = df.dropna(axis=1, how='any')\n",
    "df_2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c7396df",
   "metadata": {},
   "source": [
    "Lovely dataframe, isn't it?\n",
    "\n",
    "So, what happened? Well, we dropped every column in the initial dataframe that had any `NaN`s, which was all of them.\n",
    "\n",
    "#### Below, try to write code that drops every row that is all `NaN`s and then modifies that dataframe by dropping all columns with any `NaN`s. \n",
    "When you use `head` you should see that only the ID column remains, since by dropping all the all-`NaN` rows first you will remove all the `NaN`s from ID. Use that to check that you have done this correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04bd9dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f62c931",
   "metadata": {},
   "source": [
    "We might also want to drop `NaN`s in some columns but not others. The `subset` argument of `dropna` allows us to specify some columns to drop. The code below drops NaNs only in Heart Rate Max, and then shows us this to verify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a35d213",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.dropna(subset=['Heart Rate Max'])\n",
    "\n",
    "print(df2['Heart Rate Max'].isna().sum())\n",
    "df2.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152986c9",
   "metadata": {},
   "source": [
    "#### Below, write code that drops all rows where either ID or Measurement Device is NaN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5363abb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f27d55e",
   "metadata": {},
   "source": [
    "### Filling in Blank Cells"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "627d3b8c",
   "metadata": {},
   "source": [
    "Another useful tool is `fillna`. This lets us replace `NaN` with some value. Obviously, whether this is a good idea depends on what you're doing with the dataframe. However, let's imagine that it is a good idea to replace missing values in our data with 0. The code below will do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a82b14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.fillna(0)\n",
    "df2.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409ab5d0",
   "metadata": {},
   "source": [
    "We can see that what was previously `NaN` is now 0. `fillna` has arguments much like `dropna`. The only required one is `value`, which is the first one, that I set to 0 here. This is the value to fill in. Like several previous methods `fillna` also takes the `inplace` argument (defaulting to `False`). \n",
    "\n",
    "In the example above everything was being filled to 0. However, what if we want to fill `NaN` with 150 in the Heart Rate Max column and 100 in the Heart Rate Avg column? Then we pass `fillna` a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26c262d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.fillna({'Heart Rate Max': 150, 'Heart Rate Avg': 100})\n",
    "df2.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37ac047",
   "metadata": {},
   "source": [
    "#### For practice, write code that will fill NaNs in Measurement Device, and only Measurement Device, with some default value. As always, use head to check your work. Also, leave inplace alone, or your dataframe will be changed and that may impact the exercises below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eccd289",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc77941",
   "metadata": {},
   "source": [
    "There are some advanced uses of `fillna` which you can see in the documentation that the pandas project provides. For instance, passing a `limit` will only fill that many `NaN`s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a012e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.fillna(0, limit=1)\n",
    "df2.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b41cf00",
   "metadata": {},
   "source": [
    "As you should be able to see above, this only filled the first `NaN` with a zero, and left subsequent ones alone.\n",
    "\n",
    "A more useful advanced case is dynamic fills. As we discussed in the Pandas Data Structures notebook, you can create a new column by setting it equal to some simple mathematical expression involving other columns. So, for instance, `df['Heart Rate Midpoint'] = (df['Heart Rate Max'] + df['Heart Rate Min']) / 2` would create a column called Heart Rate Midpoint which was the average of Heart Rate Max and Heart Rate Min. We can also use these expressions in `fillna`. In the code below, we'll fill any missing values in Heart Rate Avg with the average of the maximum and minimum heart rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1bf5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.fillna({'Heart Rate Max': 150, 'Heart Rate Avg': (df['Heart Rate Max'] + df['Heart Rate Min']) / 2})\n",
    "df2.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040ad6f3",
   "metadata": {},
   "source": [
    "Try this yourself.\n",
    "\n",
    "#### Below, write code that fills NaNs in Heart Rate Min with a number that is 15 below the corresponding Heart Rate Avg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c14264",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93afce17",
   "metadata": {},
   "source": [
    "### Dealing with Duplicates\n",
    "Sometimes you get duplicated data. This is especially common if you have made a dataset yourself by combining data from several sources, but we need to deal with it. \n",
    "\n",
    "First, let's make a mess."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6183eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dup_dict = {'ID': [], 'Measurement Device': [], 'Heart Rate Max': [], 'Heart Rate Min': [], 'Heart Rate Avg': [],\n",
    "              'Duration of exercise (min)': [], 'Exercise Type': []}\n",
    "\n",
    "for index, row in df.dropna().iterrows():\n",
    "    for n in range(0, random.choice([1, 1, 1, 2, 3])):\n",
    "        dup_dict['ID'].append(int(row['ID']))\n",
    "        dup_dict['Measurement Device'].append(row['Measurement Device'])\n",
    "        dup_dict['Heart Rate Min'].append(row['Heart Rate Min'])\n",
    "        dup_dict['Heart Rate Max'].append(row['Heart Rate Max'])\n",
    "        dup_dict['Heart Rate Avg'].append(row['Heart Rate Avg'])\n",
    "        dup_dict['Duration of exercise (min)'].append(row['Duration of exercise (min)'])\n",
    "        dup_dict['Exercise Type'].append(row['Exercise Type'])\n",
    "\n",
    "dup_frame = pd.DataFrame(dup_dict)\n",
    "dup_frame.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72314e56",
   "metadata": {},
   "source": [
    "The code above essentially copies df over to a new dataframe but sometimes copies a row two or three times instead of just once. The new frame, dup_frame, has duplicates.\n",
    "\n",
    "In our case, we know that while lots of the data in our dataframe could be repeated elsewhere, the ID number should not. How many unique IDs do we have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93fec850",
   "metadata": {},
   "outputs": [],
   "source": [
    "nu = dup_frame['ID'].nunique()\n",
    "\n",
    "print('Unique IDs:', nu)\n",
    "print('ID rows:', dup_frame['ID'].count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9732554d",
   "metadata": {},
   "source": [
    "`nunique` is a dataframe method that counts the number of unique entries in the given frame or series. As you can see, there are more rows that unique IDs!\n",
    "\n",
    "(A related method, `unique`, lists the unique values. If you weren't sure what exercises were listed in Exercise Type, for instance, `df['Exercise Type'].unique()` would give you that list.)\n",
    "\n",
    "So, how do we get rid of these duplicates? The method `drop_duplicates`. Like `dropna` it takes an `inplace` argument, and in the code below we're leaving that as False so we can try other things with the original frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3341e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dropped = dup_frame.drop_duplicates()\n",
    "\n",
    "nu = dropped['ID'].nunique()\n",
    "print('Unique IDs:', nu)\n",
    "print('ID rows:', dropped['ID'].count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93a02a1",
   "metadata": {},
   "source": [
    "Now the number of rows and the number of unique IDs should be the same. By default, `drop_duplicates` considers all columns and only drop rows that are duplicates across all of them. We could restrict this if we wanted. Imagine that, for some odd reason, we only wanted one entry for each measurement device and exercise type combination. The `subset` argument would let us do that by passing a list of columns to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80199a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "dropped2 = dup_frame.drop_duplicates(subset=['Measurement Device', 'Exercise Type'])\n",
    "dropped2.head(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee8993e",
   "metadata": {},
   "source": [
    "Only the first row for each combination is kept.\n",
    "\n",
    "#### Below, write a block of code that will return dup_frame minus any duplicated Heart Rate Avg entries (only). Since it's fairly unlikely that Heart Rate Avg will randomly hit exactly the same number twice the resulting number of rows should be very close to the number of rows after dropping duplicates without restriction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1e5502",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6588749",
   "metadata": {},
   "source": [
    "### Fixing Incorrect Data Types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fdda61b",
   "metadata": {},
   "source": [
    "Imagine that, for some reason, we want to know the half-length of the exercise. This should be easy. First, we'll drop all NaNs in that column, and then make a new column that is that column divided by 2.\n",
    "\n",
    "The top two rows of code are a little housekeeping. We can't divide `NaN` by 2, so we'll drop all the `NaN`s in Duration of exercise (min) and we'll have to type the name of this column a lot, so we'll rename it to Duration, which is a lot less typing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54df24c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(subset=['Duration of exercise (min)'], inplace=True)\n",
    "df.rename(columns={'Duration of exercise (min)': 'Duration'}, inplace=True)\n",
    "\n",
    "df['Half-duration'] = df['Duration'] / 2\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3d42db",
   "metadata": {},
   "source": [
    "You'll have a long error message above, but the import parts are `TypeError`, and the line that clarifies: `TypeError: unsupported operand type(s) for /: 'str' and 'int'`.\n",
    "\n",
    "(Also important: since the error occured after our housekeeping rows both of those ran and have taken effect.) \n",
    "\n",
    "Why the TypeError? Let's check the types of the two columns (called \"dtypes\"). (Actually, we'll just print the dtypes of all columns, since this is faster.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a0c76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f80f0ab",
   "metadata": {},
   "source": [
    "As you can see, the dtypes for most columns are `float64`. The number matters less, but `float` is a floating-point number (a number that can support decimal places) and an `int` is an integer. So these are number types. Duration is an `object`. Why? Well, the code that generated it saved it as text, and so while you may see a 20 in a particular cell that's not the number 20, that's the text characters for 20. You can't divide text by a number, so we get a TypeError.\n",
    "\n",
    "How do we fix this? We tell pandas what type the column should be. There are two ways to do this. The `astype` method lets us specify a column and a dtype in a dictionary. (To enable us to try several things I am running this with `inplace` as False.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83025e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_df = df.astype({'Duration': 'int64'})\n",
    "fixed_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a37be7",
   "metadata": {},
   "source": [
    "Now we should be able to run our operation on the new dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c18af2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_df['Half-duration'] = fixed_df['Duration'] / 2\n",
    "fixed_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e81c12",
   "metadata": {},
   "source": [
    "Another way to do this would be to restrict our frame to the single column we want first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5caddef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Duration (fixed)'] = df['Duration'].astype('int64')\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738310e5",
   "metadata": {},
   "source": [
    "However, this required me to know that I wanted (or could use) an `int64`. If all you want is a number the `to_numeric` function will figure out the type for you. Note that `to_numeric` is not a dataframe method, but a general pandas one, so we will write `pd.to_numeric()` not `df.to_numeric`. This also means we need to pass what we want to turn into numbers as an argument to `to_numeric`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680a64a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Duration, fixed again'] = pd.to_numeric(df['Duration'])\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d15dad4",
   "metadata": {},
   "source": [
    "You can see here that `to_numeric` has picked a number type without us having to figure it out.\n",
    "\n",
    "`to_numeric` has a useful error-handling mechanism. Let's look at this with a list that has some characters that won't become numbers correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20cb8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.to_numeric(['1', '32', 'a', '5'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f827c4",
   "metadata": {},
   "source": [
    "By default, `to_numeric` has its `errors` argument set to `raise`, which means that when it fails to convert something it stops and gives us an error (a ValueError, in this case).\n",
    "\n",
    "`errors` can also be set to `ignore`, which just doesn't do anything to invalid input, and `coerce` which sets invalud input to `NaN`. The code below shows both of these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf170bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.to_numeric(['1', '32', 'a', '5'], errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b796956e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.to_numeric(['1', '32', 'a', '5'], errors='coerce')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc83f0f4",
   "metadata": {},
   "source": [
    "In many cases, where you know you should have a number, `coerce` is the right setting, since it won't stop the code from running but considers a bad input to be a blank cell.\n",
    "\n",
    "#### Below, fix Duration properly, inplace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a235270",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c72438",
   "metadata": {},
   "source": [
    "Less frequently, you want to force a number to be an object. The ID column in our dataframe treats the IDs as numbers, but we really want them to act like labels. (The code below simply shows that they are a numeric type.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8126393d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c021843",
   "metadata": {},
   "source": [
    "There's no simple shortcut for this, but using `astype` to turn the numbers into the `object` dtype will work.\n",
    "\n",
    "#### Below, write code that turns the ID columns into the object dtype, and then verify that using df.dtypes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a832f06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6af027a",
   "metadata": {},
   "source": [
    "Time to put it all together in one final exercise! The code below will make a new row, with NaNs and some dtype issues. This row is supposed to be the day of the year (1 to 365) that the exercise took place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba30556c",
   "metadata": {},
   "outputs": [],
   "source": [
    "days = []\n",
    "for x in range(0, len(df['ID'])):\n",
    "    if random.randint(0, 5) == 0:\n",
    "        days.append(np.nan)\n",
    "    else:\n",
    "        days.append(str(random.randint(0, 364)))\n",
    "\n",
    "df['Day'] = days\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50968b03",
   "metadata": {},
   "source": [
    "Unfortunately, this row isn't 1 to 365, it follows Python conventions and is 0 to 364.\n",
    "\n",
    "#### Below, for the last exercise, make a new row that fixes the issue in Day by adding 1 to every value in Day. To do this you will need to fix two other issues as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035be83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb0d777",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Stuck? Click here to see a hint.</summary>\n",
    "\n",
    "The first issue is the `NaN`s. Drop those out of the Day column so you can do math with just numbers.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb82f2a9",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Still stuck? Click this to see a second hint.</summary>\n",
    "\n",
    "The second issue is the dtype. You'll need to change Day to a numeric dtype.\n",
    "</details>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
